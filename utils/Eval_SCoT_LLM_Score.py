
import os
import io
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, BitsAndBytesConfig, CLIPImageProcessor

import plotly.graph_objects as go
import json
import rasterio
import re
import openai
import base64
from PIL import Image
from scipy.spatial import ConvexHull, Delaunay


def main():

    ## OpenAI API (Or Qwen API and DeepSeek API)
    openai.api_key = "..."

    def encode_image(image_path):
      with open(image_path, "rb") as image_file:
        print(image_file)
        return base64.b64encode(image_file.read()).decode('utf-8')

    system_prompt_self = "You are a fair and impartial expert evaluator responsible for scoring the explainability, trustworthiness (convincingness of the answer), and factual support of the two answers generated by the 3D large language models (on a scale of 1 to 10, with decimals allowed). "

    prompt_base_self = """
    (1) Explainability refers to the degree to which the model’s output, particularly its reasoning process, is clear, well-structured, and easily understandable to human evaluators. High explainability means that each reasoning step can be followed logically, and the path from input to answer is transparent and explicit.
    (2) Trustworthiness measures the extent to which the model’s output, including both the reasoning process and the final answer, is convincing and credible to human evaluators. A trustworthy or persuasive answer not only provides the correct information, but also presents it in a manner that increases user confidence in the correctness and reliability of the output.
    (3) Faithfulness to Facts assesses whether the reasoning steps and final answer produced by the model are strictly grounded in the input information and verifiable facts, without introducing hallucinations, unsupported claims, or factual errors. High faithfulness indicates that all reasoning and conclusions can be traced back to real, reliable evidence. 
    """

    prompt_final_self = """The output must strictly adhere to the following format (where X.X represents a numerical score between 1 and 10, with one decimal place allowed):
    {
    Answer-1 Explainability: X.X
    Answer-1 Trustworthiness: X.X
    Answer-1 Faithfulness: X.X
    Answer-2 Explainability: X.X
    Answer-2 Trustworthiness: X.X
    Answer-2 Faithfulness: X.X
    }"""

    SCANREFER_1 = json.load(open("./outputs/ValResults_NoCoT/SceneAnalysis.json")) # change this
    SCANREFER_2 = json.load(open("./outputs/ValResults_CoT/SceneAnalysis.json")) # change this
    file_path = "./SceneAnalysis_LLM_Score.txt" # change this

    scanrefer_1, scanrefer_2, all_scene_list= get_scanrefer(SCANREFER_1, SCANREFER_2, -1)
    scanrefer = {"train": scanrefer_1, "val": scanrefer_2}
    scanrefer_1 = scanrefer["train"]
    scanrefer_2 = scanrefer["val"]

    print("The total val sample is: ",len(scanrefer))

    K = 5 + 1

    for i in range(len(scanrefer_1)):

        idx = i
        print(idx)

        scene_id = scanrefer_1[idx]["scene_id"]
        question = scanrefer_1[idx]["prompt"]
        question_answer_1 = scanrefer_1[idx]["pred"]
        question_answer_2 = scanrefer_2[idx]["pred"]
        GT = scanrefer_1[idx]["ref_captions"][0]

        match = re.search(r"<Answer>(.*?)</Answer>", question_answer_2, re.DOTALL)

        if match:
            answer_text = match.group(1).strip()
            question_answer_2 = answer_text


        prompt_question = "The query is: <Question-Start> " + question + "<Question-End>. " + "The ground truth answer is: <Start-Answer-GT> " + GT + "<End-Answer-GT>. "

        prompt_answer_1 = "The Answer-1 is: <Start-Answer-1> " + question_answer_1 + "<End-Answer-1>. "

        prompt_answer_2 = "The Answer-2  is: <Start-Answer-2> " + question_answer_2 + "<End-Answer-2>. "

        # object
        bbox_file = os.path.join("./SCoT_Dataset/annotations/scans_bbox", scene_id+"_bbox.json")
        raster_file = os.path.join("./SCoT_Dataset/annotations/scans_tif", scene_id+".tif")

        with open(bbox_file, 'r') as json_file:
            bbox = json.load(json_file)
        
        instance_bboxes = []
        instance_bbox = []
        instance_labels = []
        object_name = "unknown"

        # bbox & name
        for i in range(len(bbox['bboxes'])):
            bbox_instance = np.array(bbox['bboxes'][i]['bbox'])
            instance_bboxes.append(bbox_instance)

        instance_bboxes = np.stack(instance_bboxes)
        instance_labels = instance_bboxes[:, -1]

        min_first_column = np.min(instance_bboxes[:, 0] - instance_bboxes[:, 3])
        min_second_column = np.min(instance_bboxes[:, 1] - instance_bboxes[:, 4])

        instance_bboxes[:, 0] -= min_first_column
        instance_bboxes[:, 1] -= min_second_column

        cand_instance_ids = [cand_id for cand_id in np.unique(instance_labels) if cand_id != -100]

        with rasterio.open(raster_file) as src:
            image = src.read()

        instance_bboxes_pixel = np.copy(instance_bboxes)
        instance_bboxes_pixel[:,0] = instance_bboxes_pixel[:,0] / 0.005
        instance_bboxes_pixel[:,1] = instance_bboxes_pixel[:,1] / 0.005
        instance_bboxes_pixel[:,3] = instance_bboxes_pixel[:,3] / 0.005
        instance_bboxes_pixel[:,4] = instance_bboxes_pixel[:,4] / 0.005

        arr = instance_bboxes[:,-1]
        image_with_rectangle = image[:, :, :]
        image_np = np.transpose(image_with_rectangle, (1, 2, 0)).copy()
        image_with_rectangle = np.transpose(image_np, (2, 0, 1)) 
        local_image = image_with_rectangle
        local_image = np.transpose(local_image, (1, 2, 0))
        image = Image.fromarray(local_image)
        
        image_source = io.BytesIO()
        image.save(image_source, format='PNG')
        image_source.seek(0)
        base64_image = base64.b64encode(image_source.read()).decode('utf-8')
        prompt_all = prompt_base_self + prompt_question + prompt_answer_1 + prompt_answer_2 + prompt_final_self

        messages = [
            {
            "role": "system",
            "content":system_prompt_self
            },
            {
            "role":"user",
            'content':[
              {
                'type':'text',
                'text':prompt_all,
              },
              {
                'type':'image_url',
                'image_url':{
                  'url':f"data:image/jpeg;base64,{base64_image}",
                  "detail": "high"
                }
              }
            ]
        }]

        response = openai.ChatCompletion.create(
              model = 'gpt-5.1',
              messages = messages,
              temperature=1.0,
              top_p=1.0,
              max_tokens=1024,
              n=1,
            )
        
        outputs_1 = response.choices[0].message.content

        print("The Output is: " + outputs_1)
        with open(file_path, 'a', encoding='utf-8') as file:
            file.write("order: " + str(idx) + ", scene_id: " + scene_id + "\n")
            file.write("Score: "+ outputs_1 + "\n")

    print("Completed")


def get_scanrefer(scanrefer_train, scanrefer_val, num_scenes, train_scenes_to_use=None, val_scenes_to_use=None):
    # get initial scene list
    if train_scenes_to_use is not None:
        train_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_train if data["scene_id"] in train_scenes_to_use])))
    else:
        train_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_train])))
        
    if val_scenes_to_use is not None:
        val_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_val if data["scene_id"] in val_scenes_to_use])))
    else:        
        val_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_val])))
        
    if num_scenes == -1:
        num_scenes = len(train_scene_list)
    else:
        assert len(train_scene_list) >= num_scenes
        
    # slice train_scene_list
    train_scene_list = train_scene_list[:num_scenes]

    # filter data in chosen scenes
    new_scanrefer_train = []
    for data in scanrefer_train:
        if data["scene_id"] in train_scene_list:
            new_scanrefer_train.append(data)

    new_scanrefer_val = []
    for data in scanrefer_val:
        if data["scene_id"] in val_scene_list:
            new_scanrefer_val.append(data)

    # all scanrefer scene
    all_scene_list = train_scene_list + val_scene_list

    print("train on {} samples and val on {} samples".format(len(new_scanrefer_train), len(new_scanrefer_val)))

    return new_scanrefer_train, new_scanrefer_val, all_scene_list

def one_hot(length, position):
    zeros = [0 for _ in range(length)]
    zeros[position] = 1
    zeros = np.array(zeros)
    return zeros


def shuffle_items_with_indices(lst):
    indices = list(range(len(lst)))  
    random.shuffle(indices) 
    shuffled_items = [lst[idx] for index, idx in enumerate(indices)]
    return shuffled_items, indices


def extract_subimage(image, center_x, center_y, width, height):

    start_x = max(center_x - width // 2, 0)
    end_x = min(center_x + width // 2, image.shape[2])
    start_y = max(center_y - height // 2, 0)
    end_y = min(center_y + height // 2, image.shape[1])
    subimage = image[:,  start_x:end_x,start_y:end_y]
    
    return subimage

def find_rows_with_value(arr, value):
    rows_with_value = np.where(arr == value)[0]
    return rows_with_value

def find_nearest_instances(arr, objectID, K):
    object_row = arr[arr[:, -1] == objectID]
    if object_row.shape[0] == 0:
        return []
    object_xyz = object_row[:, :3]
    distances = np.sqrt(np.sum((arr[:, :3] - object_xyz) ** 2, axis=1))
    distances[arr[:, -1] == objectID] = np.inf
    nearest_indices = np.argsort(distances)[:K]
    return arr[nearest_indices, -1]

if __name__ == "__main__":
    main()
